{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/christabs27/Linear-Regression-for-Heights/blob/main/Copy_of_Titanic_Machine_Learning_Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tyzy-R6SQ6K"
      },
      "source": [
        "# Machine Learning: Final Project\n",
        "\n",
        "### Predicting Survival on the *Titanic*\n",
        "\n",
        "The final project is intended to simulate participation in a Kaggle competition. Your challenge is to build the most accurate model for predicting which passangers would survive the sinking of the *Titanic*. The ***Titanic Machine Learning Final Project.ipynb*** Colab notebook provides some guidance for tackling the project and suggests some things to think about as you get started. However, many of the model-building decisions are left up to you. \n",
        "**Note**: Use comments in your code and text blocks to explain your decisions and results.\n",
        "\n",
        "### Build a Pipeline for a Kaggle Competition!\n",
        "\n",
        "Kaggle was started in 2010 as a platform for machine learning competitions, which aim to identify how best to optimize supervised learning problems. These initiatives offer a two-way benefit. They help companies improve their internal algorithms and they provide prospective data professionals opportunities to prove their worth.\n",
        "\n",
        "Though Kaggle usually has a singular aim of maximizing a specific metric, the idea of finding the best possible algorithm and furthermore optimizing its hyperparameters is the daily task of a data scientist. Moreover, success in Kaggle can be great for a future resume (since your information is saved on their site).\n",
        "\n",
        "Obviously, the timeframe for this lesson is not realistic in terms of a typical Kaggle workflow, as competitors spend weeks or even months optimizing every piece of an algorithm they can. However, you can get started with preliminary testing and use these principles to enter your own Kaggle competitions in the future!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb9Fw_MKSRGj"
      },
      "source": [
        "# Step 1: Importing Libraries\n",
        "\n",
        "It is best practice to import all libraries and packages early in the process.\n",
        "\n",
        "You'll probably want to import Pandas plus some packages from scikit-learn.\n",
        "\n",
        "| Type | Path | Regression | Classification |\n",
        "| --- | --- | --- | --- |\n",
        "| **Linear Models** | `sklearn.linear_model` | `LinearRegression` | `LogisticRegression` |\n",
        "|  |  |`Ridge` | `RidgeClassifier` |\n",
        "|  |  |`Lasso` |  |\n",
        "| **K Nearest Neighbors** | `sklearn.neighbors` | `KNeighborsRegressor` | `KNeighborsClassifier` |\n",
        "| **Support Vector Machines** | `sklearn.svm.` | `SVR` | `SVC` |\n",
        "| **Naive Bayes** |  `sklearn.naive_Bayes` |  |`CategoricalNB` (Categorical) |\n",
        "|  |  |  | `MultinomialNB` (Sentiment Analysis) |\n",
        "| **Decision Trees** | `sklearn.tree` | `DecisionTreeRegressor` | `DecisionTreeClassifier` |\n",
        "| **Ensemble - Random Forests** | `sklearn.ensemble` | `RandomForestRegressor` | `RandomForestClassifier`\n",
        "| **Ensemble - Boosting** | `sklearn.ensemble` | `AdaBoostRegressor` | `AdaBoostClassifier` |\n",
        "|  | `sklearn.ensemble` | `GradientBoostRegressor` | `GradientBoostClassifier` |\n",
        "\n",
        "\n",
        "\n",
        "| Type | Path | Package |\n",
        "| --- | --- | --- |\n",
        "| Preprocessing | `sklearn.preprocessing` | `StandardScaler` |\n",
        "| |`sklearn.preprocessing` | `MinMaxScaler` |\n",
        "| |`sklearn.preprocessing` | `MaxAbsScaler` |\n",
        "| Model Selection - Splitting| `sklearn.model_selection` | `train_test_split` |\n",
        "| Model Selection - Grid Search | `sklearn.model_selection` | `GridSearchCV` |\n",
        "| Model Selection - Scoring | `sklearn.model_selection` | `cross_val_score` |\n",
        "| Metrics | `sklearn.metrics` | `confusion_matrix` |\n",
        "\n",
        "\n",
        "**Note**: Use comments in your code and text blocks to explain your decisions and results.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-q3XVj2acth"
      },
      "source": [
        "#Step 1\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import cross_val_score\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd1PYNlBaZUP"
      },
      "source": [
        "#Step 2:  Load the `Titanic.csv` Data\n",
        "You may want to refer back to one of your previous Colab notebooks to copy the Google Import code.\n",
        "\n",
        "**Note**: Use comments in your code and text blocks to explain your decisions and results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2YgjFdUbh6d",
        "outputId": "cf853efa-ec31-4b23-9964-a1ad79f28b81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "#Step 2\n",
        "from google.colab import files\n",
        "titanic = files.upload()\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1d54a619-f776-4e1a-8e24-9ee33f97283a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1d54a619-f776-4e1a-8e24-9ee33f97283a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Titanic.csv to Titanic.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "titanic = pd.read_csv('Titanic.csv')\n",
        "titanic.head\n"
      ],
      "metadata": {
        "id": "pk_CUpOohc85",
        "outputId": "cc8e6122-650a-4d55-81d6-bc5e604689e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.head of      PassengerId  Survived  Pclass  \\\n",
              "0              1         0       3   \n",
              "1              2         1       1   \n",
              "2              3         1       3   \n",
              "3              4         1       1   \n",
              "4              5         0       3   \n",
              "..           ...       ...     ...   \n",
              "886          887         0       2   \n",
              "887          888         1       1   \n",
              "888          889         0       3   \n",
              "889          890         1       1   \n",
              "890          891         0       3   \n",
              "\n",
              "                                                  Name     Sex   Age  SibSp  \\\n",
              "0                              Braund, Mr. Owen Harris    male  22.0      1   \n",
              "1    Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
              "2                               Heikkinen, Miss. Laina  female  26.0      0   \n",
              "3         Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
              "4                             Allen, Mr. William Henry    male  35.0      0   \n",
              "..                                                 ...     ...   ...    ...   \n",
              "886                              Montvila, Rev. Juozas    male  27.0      0   \n",
              "887                       Graham, Miss. Margaret Edith  female  19.0      0   \n",
              "888           Johnston, Miss. Catherine Helen \"Carrie\"  female   NaN      1   \n",
              "889                              Behr, Mr. Karl Howell    male  26.0      0   \n",
              "890                                Dooley, Mr. Patrick    male  32.0      0   \n",
              "\n",
              "     Parch            Ticket     Fare Cabin Embarked  \n",
              "0        0         A/5 21171   7.2500   NaN        S  \n",
              "1        0          PC 17599  71.2833   C85        C  \n",
              "2        0  STON/O2. 3101282   7.9250   NaN        S  \n",
              "3        0            113803  53.1000  C123        S  \n",
              "4        0            373450   8.0500   NaN        S  \n",
              "..     ...               ...      ...   ...      ...  \n",
              "886      0            211536  13.0000   NaN        S  \n",
              "887      0            112053  30.0000   B42        S  \n",
              "888      2        W./C. 6607  23.4500   NaN        S  \n",
              "889      0            111369  30.0000  C148        C  \n",
              "890      0            370376   7.7500   NaN        Q  \n",
              "\n",
              "[891 rows x 12 columns]>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Z0J_ay1SRPz"
      },
      "source": [
        "#Step 3: Split the Data\n",
        "\n",
        "The next step is to separate the target column from the feature matrix and perform a train/test split. \n",
        "\n",
        "*   What is the target and what are the features in the data?\n",
        "*   Are there any features that you want to drop?\n",
        "*   Is there any feature engineering that you need to do?\n",
        "\n",
        "**Note**: Use comments in your code and text blocks to explain your decisions and results.\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tmaj7Zc-kBTL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "c65037c6-a2a5-4a1f-f9be-0cf27b033c8c"
      },
      "source": [
        "#Step 4\n",
        "# Going to clean the data first since there are a lot nulls\n",
        "\n",
        "def clean(titanic):\n",
        "  Titanic = titanic.drop([\"Ticket\", \"Cabin\", \"PassengerID\",\"Name\"])\n",
        "\n",
        "  cols =[\"SipSp\", \"Parch\", \"Fare\", \"Age\"] \n",
        "  for col in cols: \n",
        "    data[col].fillna(data[col]).mean(), inplace=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-22-74e1ce6d73bd>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    data[col].fillna(data[col]).mean(), inplace=True)\u001b[0m\n\u001b[0m                                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "cols =['SipSp', 'Parch, 'Fare', 'Age']\n",
        "for col in cols:\n",
        "    data[col].fillna(data[col]).mean(), inplace=True\n"
      ],
      "metadata": {
        "id": "gCVIpXg3tqls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imp_mean', SimpleImputer(missing_values=np.nan,\n",
        "strategy='mean'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "id": "tcwxS-m0qXLn",
        "outputId": "6755985b-1ed4-4609-c3a3-a9ef696ef19e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-d6f8558aa951>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    imp_mean', SimpleImputer(missing_values=np.nan,\u001b[0m\n\u001b[0m                                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imp_mean', SimpleImputer(missing_values=np.nan, strategy='mean'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "id": "4NHXx-Wrts96",
        "outputId": "069a1b64-bce5-4c92-d8a5-7e99840324ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-20-f76354336319>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    imp_mean', SimpleImputer(missing_values=np.nan, strategy='mean'\u001b[0m\n\u001b[0m                                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
      ],
      "metadata": {
        "id": "5rqZCkyIuVWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0lndw6BuQUW"
      },
      "source": [
        "#Step 4: Clean and Preprocess the Data\n",
        "\n",
        "Use the code block below to clean and preprocess your data. Some considerations you may want to think about include the following:  \n",
        "*  Are there any missing values that need to be imputed?\n",
        "*  Do you need to encode any categorical features?\n",
        "*  Do you need to standardize any quantitative features?\n",
        " \n",
        "**Note**: Use comments in your code and text blocks to explain your decisions and results.\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 3\n",
        "\n",
        "\n",
        "X_train_s = StandardScaler().fit_transform(X_train)\n",
        "X_train_s = StandardScaler().fit_transform(X_train)\n",
        "X_test_s =  StandardScaler().fit_transform(X_test)"
      ],
      "metadata": {
        "id": "GTZ4KkS9um7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcqBl3tqSRXM"
      },
      "source": [
        "#Step 5: Build the Baseline Model\n",
        "\n",
        "Ideally, you will want to set a baseline algorithm to build off of. The most logical start is *linear regression* for *regression* and *logistic regression* for *classification*, as they are the basis for their respective algorithms.\n",
        "\n",
        "Once you have the baseline set, you will want to choose an algorithm that surpasses the baseline.\n",
        "\n",
        "Select a baseline model and fit it to your data.\n",
        "\n",
        "**Note**: Use comments in your code and text blocks to explain your decisions and results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-He6s8fukCDO"
      },
      "source": [
        "# Step 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rkx6qXH3vAon"
      },
      "source": [
        "#Step 6: Evaluate the Baseline Model\n",
        "\n",
        "Use cross-validation to calculate the appropriate model evaluation metric. \n",
        "\n",
        "Is your model doing a good job fitting the data?  \n",
        "\n",
        "If you have ideas for how to improve your model fit, go back and make those changes to earlier steps.\n",
        "\n",
        "**Note**: Use comments in your code and text blocks to explain your decisions and results.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 6"
      ],
      "metadata": {
        "id": "0eTjP2jeu6v7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDkRu0kWY6q6"
      },
      "source": [
        "# Step 7: Fit the Data to at Least One Other Model\n",
        "\n",
        "Select one (or more) other appropriate model and use it to model the data. Calculate the cross-validation accuracy of each model. \n",
        "\n",
        "**Note**: Use comments in your code and text blocks to explain your decisions and results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mf-2sSGRY7Qr"
      },
      "source": [
        "#Step 7"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ep2NIZeex_yj"
      },
      "source": [
        "# Step 8: Evaluate Your Best Model\n",
        "\n",
        "Evaluate your best model using the test set. \n",
        "\n",
        "*   Which model fit the data best?\n",
        "*   What was the best accuracy you were able to achieve?  \n",
        "\n",
        "**Note**: Use comments in your code and text blocks to explain your decisions and results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2u20nsPshZb2"
      },
      "source": [
        "# Step 8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 9: Final Reporting\n",
        "\n",
        "Summarize your model building process:  \n",
        "* How did you identify the model target and features?  \n",
        "* What steps did you take to prepare the data for modeling?  \n",
        "* Which baseline model did you choose and why? How did you evaluate the model's performance?  \n",
        "* Which other model(s) did you choose and why? How did you evaluate the model's performace?  \n",
        "* What was the best model you developed? How well did the model perform on the test data?"
      ],
      "metadata": {
        "id": "JYcXAvwtyrnA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 9:"
      ],
      "metadata": {
        "id": "6x9-IRv6zZlO"
      }
    }
  ]
}