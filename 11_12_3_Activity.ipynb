{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/christabs27/Linear-Regression-for-Heights/blob/main/11_12_3_Activity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hX4n9TsbGw-f"
      },
      "source": [
        "# Activity 11.12.3\n",
        "\n",
        "How many of us decide where to eat or what product to buy based on consumer reviews?  This is a very important source of information to both businesses and consumers.  However, there are way too many reviews being generated for humans to read and classify all of them.  Enter machine learning and natural language processing.\n",
        "\n",
        "In this lesson, you will build on your base Naive Bayes model by incorporating stop words, stemming, lemmatization, and n-grams to see how they can improve model accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 1: Install the necessary packages\n",
        "Run the following code block to import the necessary packages and set up the stemmer and lemmatizer."
      ],
      "metadata": {
        "id": "eOE0IPoIQ91v"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z682XYsrjkY9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a40270ad-73b9-4706-9882-33578cd75718"
      },
      "source": [
        "#Step 1\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "import nltk\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from textblob import TextBlob\n",
        "import re\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "porter_stemmer = PorterStemmer()\n",
        "from nltk import word_tokenize         \n",
        "from nltk.stem import WordNetLemmatizer \n",
        "\n",
        "def stemming_tokenizer(str_input):\n",
        "    words = re.sub(r\"[^A-Za-z0-9\\-]\", \" \", str_input).lower().split()\n",
        "    words = [porter_stemmer.stem(word) for word in words]\n",
        "    return words\n",
        "\n",
        "class LemmaTokenizer:\n",
        "    def __init__(self):\n",
        "        self.wnl = WordNetLemmatizer()\n",
        "    def __call__(self, doc):\n",
        "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVDbdDBDQVoS"
      },
      "source": [
        "##Step 2: Download and save the `yelp_labeled.txt` data set from the class resources  \n",
        "\n",
        "Make a note of where you saved the file on your computer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63UbfVKeQfsl"
      },
      "source": [
        "##Step 3: Upload the `yelp_labeled.txt` running the following code block \n",
        "\n",
        "When prompted, navigate to and select the `yelp_labeled.txt` data set where you saved it on your computer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxSC_MgQQUis",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "b2e4b59d-8521-45ea-b7fa-e52a042e0b99"
      },
      "source": [
        "#Step 3\n",
        "\n",
        "from google.colab import files\n",
        "yelp_labeled = files.upload()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-99842782-daf1-4050-a083-38e4d5493476\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-99842782-daf1-4050-a083-38e4d5493476\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving yelp_labeled.txt to yelp_labeled.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3aT3Y28TLRX"
      },
      "source": [
        "## Step 4: Create a Pandas Dataframe from the `.txt` file\n",
        "* Run the following code block to create a Pandas DataFrame from the `.txt` file and name it `Yelp`.\n",
        "* The sentiment of the reviews is coded as `0` (negative) and `1` (positive).  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 4\n",
        "\n",
        "Yelp = pd.read_csv('yelp_labeled.txt',delimiter='\\t',header=None)\n",
        "Yelp.rename(columns={0:'Review',1:'Sentiment'}, inplace=True)\n",
        "print(Yelp.head())"
      ],
      "metadata": {
        "id": "7hpGXDOkwWk-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01dd947a-4797-40d6-e24b-c3b5d0524d3c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              Review  Sentiment\n",
            "0                            Wow...Loved this place.          1\n",
            "1                                 Crust is not good.          0\n",
            "2          Not tasty and the texture was just nasty.          0\n",
            "3  Stopped by during the late May bank holiday of...          1\n",
            "4  The selection on the menu was great and so wer...          1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 5: Instantiate and fit `CountVectorizer` that removes stop words from the English language\n",
        "* Transform the review data and save the results as `X`.\n",
        "* Create a target vector called `y` from `Yelp['Sentiment']`.\n",
        "* This can be done by running the following code:\n",
        "\n",
        "```\n",
        "Vectorizer = CountVectorizer(stop_words={'english'})\n",
        "\n",
        "X = Vectorizer.fit_transform(Yelp['Review'])\n",
        "\n",
        "y = Yelp['Sentiment']\n",
        "```\n"
      ],
      "metadata": {
        "id": "x6ETJREm_dZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 5\n",
        "Vectorizer = CountVectorizer(stop_words={'english'})\n",
        "\n",
        "X = Vectorizer.fit_transform(Yelp['Review'])\n",
        "\n",
        "y = Yelp['Sentiment']\n",
        "\n"
      ],
      "metadata": {
        "id": "eIVIHbiNwZso"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 6: Model the data and calculate accuracy\n",
        "* Run the following code block to split the data into training and test sets, model the data that has been pre-processed to use English stop words, and calculate the 10-fold cross-validation accuracy.\n",
        "* What is the accuracy of the model now? How does it compare to the naive model, which had 79% accuracy?"
      ],
      "metadata": {
        "id": "L_zWz_YqBT4z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 6\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "yelp_review = MultinomialNB()\n",
        "yelp_review.fit(X_train,y_train)\n",
        "\n",
        "scores = cross_val_score(yelp_review, X_train, y_train, cv=10)\n",
        "\n",
        "print(scores.mean())"
      ],
      "metadata": {
        "id": "a3qASmlfxBe1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff404cc1-0e5f-4069-b6c3-df93931e5a20"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7906666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Answer:"
      ],
      "metadata": {
        "id": "bSqgVq41-GpJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 7: Instantiate and fit `CountVectorizer` to stem the words\n",
        "* Transform the review data and save the results as `X_2`.\n",
        "* This can be done by running the following code:\n",
        "\n",
        "```\n",
        "Vectorizer2 = CountVectorizer(tokenizer=stemming_tokenizer)\n",
        "\n",
        "X_2 = Vectorizer2.fit_transform(Yelp['Review'])\n",
        "\n",
        "```\n"
      ],
      "metadata": {
        "id": "-OTSop4Tm27l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 7:\n",
        "Vectorizer2 = CountVectorizer(tokenizer=stemming_tokenizer)\n",
        "\n",
        "X_2 = Vectorizer2.fit_transform(Yelp['Review'])\n",
        "\n"
      ],
      "metadata": {
        "id": "WaaUBiBmmVhr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 8: Model the data and calculate accuracy\n",
        "* Run the following code block to split the data into training and test sets, model the data that has been pre-processed to remove stop words and stem the words, and calculate the 10-fold cross-validation accuracy.\n",
        "* What is the accuracy of the model now? How does it compare to the naive model, which had 79% accuracy?"
      ],
      "metadata": {
        "id": "ylmCpzv8_lKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 8\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_2, y, test_size=0.25, random_state=42)\n",
        "\n",
        "yelp_review = MultinomialNB()\n",
        "yelp_review.fit(X_train,y_train)\n",
        "\n",
        "scores = cross_val_score(yelp_review, X_train, y_train, cv=10)\n",
        "\n",
        "print(scores.mean())"
      ],
      "metadata": {
        "id": "cikaY6nE_lKb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f2a8c8e-76a3-4eff-f838-0eff8c26c392"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7959999999999999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Answer:\n",
        "\n"
      ],
      "metadata": {
        "id": "lJXenjNWauII"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 9: Instantiate and fit `CountVectorizer` to lemmatize the words\n",
        "* Transform the review data and save the results as `X_3`.\n",
        "* This can be done by running the following code:\n",
        "\n",
        "```\n",
        "Vectorizer3 = CountVectorizer(tokenizer=LemmaTokenizer())\n",
        "\n",
        "X_3 = Vectorizer3.fit_transform(Yelp['Review'])\n",
        "\n",
        "```\n"
      ],
      "metadata": {
        "id": "PDNmOyNeX7KX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 9\n",
        "Vectorizer3 = CountVectorizer(tokenizer=LemmaTokenizer())\n",
        "\n",
        "X_3 = Vectorizer3.fit_transform(Yelp['Review'])\n",
        "\n"
      ],
      "metadata": {
        "id": "rc6VzL4_IST9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 10: Model the data and calculate accuracy\n",
        "* Run the following code block to split the data into training and test sets, model the data that has been pre-processed to remove stop words, lemmatize the data, and calculate the 10-fold cross-validation accuracy.\n",
        "* What is the accuracy of the model now? How does it compare to the naive model, which had 79% accuracy?  How does the accuracy compare to the model where we stemmed the data?"
      ],
      "metadata": {
        "id": "M7ObyFWtYKlY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 10\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_3, y, test_size=0.25, random_state=42)\n",
        "\n",
        "yelp_review = MultinomialNB()\n",
        "yelp_review.fit(X_train,y_train)\n",
        "\n",
        "scores = cross_val_score(yelp_review, X_train, y_train, cv=10)\n",
        "\n",
        "print(scores.mean())"
      ],
      "metadata": {
        "id": "OCZTlM8eYUFB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6c1a172-1d7a-4aa0-9f55-eb20c2fa1d1d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7853333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Answer:\n",
        "\n"
      ],
      "metadata": {
        "id": "MOZkfgsXbHXw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 11: Instantiate and fit `CountVectorizer` to create n-grams up to size 3\n",
        "* Transform the review data and save the results as `X_4`.\n",
        "* This can be done by running the following code:\n",
        "\n",
        "```\n",
        "Vectorizer4 = CountVectorizer(ngram_range=(1,3))\n",
        "\n",
        "X_4 = Vectorizer4.fit_transform(Yelp['Review'])\n",
        "\n",
        "```\n"
      ],
      "metadata": {
        "id": "-hmK4B2RdITo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 11\n",
        "Vectorizer4 = CountVectorizer(ngram_range=(1,3))\n",
        "\n",
        "X_4 = Vectorizer4.fit_transform(Yelp['Review'])\n"
      ],
      "metadata": {
        "id": "amivTV_cdYOx"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 12: Model the data and calculate accuracy\n",
        "* Run the following code block to split the data into training and test sets, model the data that has been pre-processed into n-grams, and calculate the 10-fold cross-validation accuracy.\n",
        "* What is the accuracy of the model now? How does it compare to the naive model, which had 79% accuracy?  How does the accuracy compare to the model where we stemmed the data?"
      ],
      "metadata": {
        "id": "ENDC7EvrdiHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 12:\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_4, y, test_size=0.25, random_state=42)\n",
        "\n",
        "yelp_review = MultinomialNB()\n",
        "yelp_review.fit(X_train,y_train)\n",
        "\n",
        "scores = cross_val_score(yelp_review, X_train, y_train, cv=10)\n",
        "\n",
        "print(scores.mean())"
      ],
      "metadata": {
        "id": "BQG3_vJ9rewb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d1821a5-e17c-4bcc-87d8-4bf68aa5e9be"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.784\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Answer:\n",
        "\n"
      ],
      "metadata": {
        "id": "frUIkVQ7rewd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 13: Try both stemming and using n-grams\n",
        "* Run the following code block to train and evaluate a model that uses both stemming and n-grams up to tri-grams.\n",
        "* What is the accuracy of the model now? How does it compare to the naive model, which had 79% accuracy? How does the accuracy compare to the model where we stemmed the data?\n",
        "\n"
      ],
      "metadata": {
        "id": "eJboe4u91AUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 13\n",
        "\n",
        "Vectorizer5 = CountVectorizer(ngram_range=(1,3), tokenizer=stemming_tokenizer)\n",
        "\n",
        "X_5 = Vectorizer5.fit_transform(Yelp['Review'])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_5, y, test_size=0.25, random_state=42)\n",
        "\n",
        "yelp_review = MultinomialNB()\n",
        "yelp_review.fit(X_train,y_train)\n",
        "\n",
        "scores = cross_val_score(yelp_review, X_train, y_train, cv=10)\n",
        "\n",
        "print(scores.mean())"
      ],
      "metadata": {
        "id": "rynpHouL1DU1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cfc6dd5-f32f-4679-f8ce-0b8df0931bc4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7920000000000001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Answer:\n",
        "\n"
      ],
      "metadata": {
        "id": "2CUC4Yyr12DC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 14: Evaluate the model with stemming and n-grams on the test data\n",
        "* Run the following code block to evaluate the model using the test data.\n",
        "* What is the testing accuracy?\n",
        "\n"
      ],
      "metadata": {
        "id": "T8SD4l_X2Hq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 14\n",
        "\n",
        "yelp_review.score(X_test,y_test)"
      ],
      "metadata": {
        "id": "Ucq44inL2d1l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c1c85ed-169c-45cf-e437-732bf7cd6d16"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.808"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Answer:\n",
        "\n"
      ],
      "metadata": {
        "id": "xkDQrqkb2mML"
      }
    }
  ]
}