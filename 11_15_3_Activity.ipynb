{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/christabs27/Linear-Regression-for-Heights/blob/main/11_15_3_Activity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hX4n9TsbGw-f"
      },
      "source": [
        "# Activity 11.15.3\n",
        "\n",
        "In this activity, we will use reinforcement learning to teach a cart to move so that a wobbly pole balanced on top of it remains upright.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 1: Install the necessary packages\n",
        "\n",
        "\n",
        "```\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install -y xvfb ffmpeg freeglut3-dev\n",
        "!pip install 'imageio==2.4.0'\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install tf-agents[reverb]\n",
        "!pip install pyglet\n",
        "\n",
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import base64\n",
        "import imageio\n",
        "import IPython\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import pyvirtualdisplay\n",
        "import reverb\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.drivers import py_driver\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.eval import metric_utils\n",
        "from tf_agents.metrics import tf_metrics\n",
        "from tf_agents.networks import sequential\n",
        "from tf_agents.policies import py_tf_eager_policy\n",
        "from tf_agents.policies import random_tf_policy\n",
        "from tf_agents.replay_buffers import reverb_replay_buffer\n",
        "from tf_agents.replay_buffers import reverb_utils\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.specs import tensor_spec\n",
        "from tf_agents.utils import common\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "eOE0IPoIQ91v"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uyiEuw-5V3eL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z682XYsrjkY9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "ee7e9b63-47bf-44a9-df9a-b4570bd554d3"
      },
      "source": [
        "#Step 1\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install -y xvfb ffmpeg freeglut3-dev\n",
        "!pip install 'imageio==2.4.0'\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install tf-agents[reverb]\n",
        "!pip install pyglet\n",
        "\n",
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import base64\n",
        "import imageio\n",
        "import IPython\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import pyvirtualdisplay\n",
        "import reverb\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.drivers import py_driver\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.eval import metric_utils\n",
        "from tf_agents.metrics import tf_metrics\n",
        "from tf_agents.networks import sequential\n",
        "from tf_agents.policies import py_tf_eager_policy\n",
        "from tf_agents.policies import random_tf_policy\n",
        "from tf_agents.replay_buffers import reverb_replay_buffer\n",
        "from tf_agents.replay_buffers import reverb_utils\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.specs import tensor_spec\n",
        "from tf_agents.utils import common"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-d7c00a2900d5>\"\u001b[0;36m, line \u001b[0;32m12\u001b[0m\n\u001b[0;31m    import imageio\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m from __future__ imports must occur at the beginning of the file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 2: Create the environment\n",
        "* Run the following code block to create our wiggly pole and cart environment.\n",
        "* The black rectangle is the cart."
      ],
      "metadata": {
        "id": "Y4LC_egITjfi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 2\n",
        "\n",
        "display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()\n",
        "\n",
        "num_iterations = 20000 \n",
        "\n",
        "initial_collect_steps = 100  \n",
        "collect_steps_per_iteration =   1# \n",
        "replay_buffer_max_length = 100000  # \n",
        "\n",
        "batch_size = 64  \n",
        "learning_rate = 1e-3  \n",
        "log_interval = 200  # \n",
        "\n",
        "num_eval_episodes = 10  \n",
        "eval_interval = 1000  \n",
        "\n",
        "env_name = 'CartPole-v0'\n",
        "env = suite_gym.load(env_name)\n",
        "\n",
        "train_py_env = suite_gym.load(env_name)\n",
        "eval_py_env = suite_gym.load(env_name)\n",
        "\n",
        "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
        "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n",
        "\n",
        "env.reset()\n",
        "PIL.Image.fromarray(env.render())"
      ],
      "metadata": {
        "id": "hin8hcpMITm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 3: Get information from the evironment\n",
        "* Run the following code block to view the kinds of information our agent receives from the environment at each time point.  The goal is to move the cart right and left so the pole remains upright.  \n",
        "* There are two time steps shown here.  We see the pole at one time step, move the cart with the `action=` step, and then view the state of the cart at the next time step.\n",
        "* `observation` gives 4 measures of the angle and velocity of the pole and the cart.\n",
        "* `reward` gives the value of the reward given to the agent at that time point.\n",
        "* The cart can take two possible actions: `0` = move left and `1` = move right.\n",
        "* If the angle of the pole becomes too great (i.e., the pole is going to fall over) the agent *loses* and the environment is reset."
      ],
      "metadata": {
        "id": "5U4U1ZTPVN11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 3\n",
        "\n",
        "time_step = env.reset()\n",
        "print('Time step:')\n",
        "print(time_step)\n",
        "\n",
        "action = np.array(1, dtype=np.int32)\n",
        "\n",
        "next_time_step = env.step(action)\n",
        "print('Next time step:')\n",
        "print(next_time_step)"
      ],
      "metadata": {
        "id": "ppL3vhroI0aV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 4: Set up the agent\n",
        "* This is where the neural network part of reinforcement learning comes in.  The agent will learn how to move the cart in response to the angle and velocity of the pole, so it stays upright.\n",
        "* Run the following code block to create the agent."
      ],
      "metadata": {
        "id": "cEw9Lb7BXeAj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 4\n",
        "\n",
        "fc_layer_params = (100, 50)\n",
        "action_tensor_spec = tensor_spec.from_spec(env.action_spec())\n",
        "num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n",
        "\n",
        "def dense_layer(num_units):\n",
        "  return tf.keras.layers.Dense(\n",
        "      num_units,\n",
        "      activation=tf.keras.activations.relu,\n",
        "      kernel_initializer=tf.keras.initializers.VarianceScaling(\n",
        "          scale=2.0, mode='fan_in', distribution='truncated_normal'))\n",
        "\n",
        "dense_layers = [dense_layer(num_units) for num_units in fc_layer_params]\n",
        "q_values_layer = tf.keras.layers.Dense(\n",
        "    num_actions,\n",
        "    activation=None,\n",
        "    kernel_initializer=tf.keras.initializers.RandomUniform(\n",
        "        minval=-0.03, maxval=0.03),\n",
        "    bias_initializer=tf.keras.initializers.Constant(-0.2))\n",
        "q_net = sequential.Sequential(dense_layers + [q_values_layer])\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "train_step_counter = tf.Variable(0)\n",
        "\n",
        "agent = dqn_agent.DqnAgent(\n",
        "    train_env.time_step_spec(),\n",
        "    train_env.action_spec(),\n",
        "    q_network=q_net,\n",
        "    optimizer=optimizer,\n",
        "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
        "    train_step_counter=train_step_counter)\n",
        "\n",
        "agent.initialize()"
      ],
      "metadata": {
        "id": "s_arkeIiJ_9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 5: Set up a policy\n",
        "* Run the following code block to define the policy, which is the set of rules for how the agent interacts with the environment (for example, the cart can move right and left, but it can't take off and fly around).\n",
        "* The goal is to train the underlying model until the policy produces the desired outcome—keeping the pole balanced on top of the cart.\n"
      ],
      "metadata": {
        "id": "5L46Is9eZKZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 5\n",
        "\n",
        "eval_policy = agent.policy\n",
        "collect_policy = agent.collect_policy\n",
        "\n",
        "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
        "                                                train_env.action_spec())\n",
        "\n",
        "example_environment = tf_py_environment.TFPyEnvironment(\n",
        "    suite_gym.load('CartPole-v0'))\n",
        "\n",
        "time_step = example_environment.reset()\n",
        "\n",
        "random_policy.action(time_step)"
      ],
      "metadata": {
        "id": "3TGCHZr0KJWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 6: Define performance metrics\n",
        "* The agent needs to know how well it is operating in the environment.  At every time step, the agent is given a reward.  The agent's goal is to maximize the reward it gets every time it interacts with the environment.  \n",
        "* If the pole falls over very quickly, the agent will not get much of a reward.  \n",
        "* Run the following code block to build a function that calculates the average return for a policy.  \n",
        "* The output is the average return for an agent that is acting completely at random.  We can use this as a baseline to see how training improves our agent's ability to balance the pole.\n"
      ],
      "metadata": {
        "id": "SvYNyu90ayPd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 6\n",
        "\n",
        "def compute_avg_return(environment, policy, num_episodes=10):\n",
        "\n",
        "  total_return = 0.0\n",
        "  for _ in range(num_episodes):\n",
        "\n",
        "    time_step = environment.reset()\n",
        "    episode_return = 0.0\n",
        "\n",
        "    while not time_step.is_last():\n",
        "      action_step = policy.action(time_step)\n",
        "      time_step = environment.step(action_step.action)\n",
        "      episode_return += time_step.reward\n",
        "    total_return += episode_return\n",
        "\n",
        "  avg_return = total_return / num_episodes\n",
        "  return avg_return.numpy()[0]\n",
        "\n",
        "compute_avg_return(eval_env, random_policy, num_eval_episodes)"
      ],
      "metadata": {
        "id": "8KCbnm3HKR-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 7: Collect data\n",
        "Run the following code block to collect data from the environment and save it in a replay buffer that can be used to train the agent."
      ],
      "metadata": {
        "id": "XFPqYHAVqDC9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 7\n",
        "\n",
        "table_name = 'uniform_table'\n",
        "replay_buffer_signature = tensor_spec.from_spec(\n",
        "      agent.collect_data_spec)\n",
        "replay_buffer_signature = tensor_spec.add_outer_dim(\n",
        "    replay_buffer_signature)\n",
        "\n",
        "table = reverb.Table(\n",
        "    table_name,\n",
        "    max_size=replay_buffer_max_length,\n",
        "    sampler=reverb.selectors.Uniform(),\n",
        "    remover=reverb.selectors.Fifo(),\n",
        "    rate_limiter=reverb.rate_limiters.MinSize(1),\n",
        "    signature=replay_buffer_signature)\n",
        "\n",
        "reverb_server = reverb.Server([table])\n",
        "\n",
        "replay_buffer = reverb_replay_buffer.ReverbReplayBuffer(\n",
        "    agent.collect_data_spec,\n",
        "    table_name=table_name,\n",
        "    sequence_length=2,\n",
        "    local_server=reverb_server)\n",
        "\n",
        "rb_observer = reverb_utils.ReverbAddTrajectoryObserver(\n",
        "  replay_buffer.py_client,\n",
        "  table_name,\n",
        "  sequence_length=2)\n",
        "\n",
        "py_driver.PyDriver(\n",
        "    env,\n",
        "    py_tf_eager_policy.PyTFEagerPolicy(\n",
        "      random_policy, use_tf_function=True),\n",
        "    [rb_observer],\n",
        "    max_steps=initial_collect_steps).run(train_py_env.reset())\n",
        "\n",
        "dataset = replay_buffer.as_dataset(\n",
        "    num_parallel_calls=3,\n",
        "    sample_batch_size=batch_size,\n",
        "    num_steps=2).prefetch(3)\n",
        "\n",
        "iterator = iter(dataset)\n",
        "print(iterator)"
      ],
      "metadata": {
        "id": "7hpGXDOkwWk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 8: Train the agent\n",
        "* Run the following code block to train the agent based on data collected from the environment.\n",
        "* **Note**: This step will take about 5 minutes to run."
      ],
      "metadata": {
        "id": "x6ETJREm_dZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 8\n",
        "\n",
        "agent.train = common.function(agent.train)\n",
        "\n",
        "# Reset the train step.\n",
        "agent.train_step_counter.assign(0)\n",
        "\n",
        "# Evaluate the agent's policy once before training.\n",
        "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
        "returns = [avg_return]\n",
        "\n",
        "# Reset the environment.\n",
        "time_step = train_py_env.reset()\n",
        "\n",
        "# Create a driver to collect experience.\n",
        "collect_driver = py_driver.PyDriver(\n",
        "    env,\n",
        "    py_tf_eager_policy.PyTFEagerPolicy(\n",
        "      agent.collect_policy, use_tf_function=True),\n",
        "    [rb_observer],\n",
        "    max_steps=collect_steps_per_iteration)\n",
        "\n",
        "for _ in range(num_iterations):\n",
        "\n",
        "  # Collect a few steps and save to the replay buffer.\n",
        "  time_step, _ = collect_driver.run(time_step)\n",
        "\n",
        "  # Sample a batch of data from the buffer and update the agent's network.\n",
        "  experience, unused_info = next(iterator)\n",
        "  train_loss = agent.train(experience).loss\n",
        "\n",
        "  step = agent.train_step_counter.numpy()\n",
        "\n",
        "  if step % log_interval == 0:\n",
        "    print('step = {0}: loss = {1}'.format(step, train_loss))\n",
        "\n",
        "  if step % eval_interval == 0:\n",
        "    avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
        "    print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
        "    returns.append(avg_return)"
      ],
      "metadata": {
        "id": "eIVIHbiNwZso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 9: Watch the system in action\n",
        "Run the following code block to view a demo video of the cart learning how to balance the pole."
      ],
      "metadata": {
        "id": "L_zWz_YqBT4z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 9\n",
        "\n",
        "def embed_mp4(filename):\n",
        "  \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n",
        "  video = open(filename,'rb').read()\n",
        "  b64 = base64.b64encode(video)\n",
        "  tag = '''\n",
        "  <video width=\"640\" height=\"480\" controls>\n",
        "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
        "  Your browser does not support the video tag.\n",
        "  </video>'''.format(b64.decode())\n",
        "\n",
        "  return IPython.display.HTML(tag)\n",
        "\n",
        "def create_policy_eval_video(policy, filename, num_episodes=5, fps=30):\n",
        "  filename = filename + \".mp4\"\n",
        "  with imageio.get_writer(filename, fps=fps) as video:\n",
        "    for _ in range(num_episodes):\n",
        "      time_step = eval_env.reset()\n",
        "      video.append_data(eval_py_env.render())\n",
        "      while not time_step.is_last():\n",
        "        action_step = policy.action(time_step)\n",
        "        time_step = eval_env.step(action_step.action)\n",
        "        video.append_data(eval_py_env.render())\n",
        "  return embed_mp4(filename)\n",
        "\n",
        "create_policy_eval_video(agent.policy, \"trained-agent\")"
      ],
      "metadata": {
        "id": "cVR2rT_ed0vy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 10: Compare to an untrained system\n",
        "For comparison, run the following code block to show the cart moving at random."
      ],
      "metadata": {
        "id": "-OTSop4Tm27l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 10:\n",
        "\n",
        "create_policy_eval_video(random_policy, \"random-agent\")"
      ],
      "metadata": {
        "id": "WaaUBiBmmVhr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}